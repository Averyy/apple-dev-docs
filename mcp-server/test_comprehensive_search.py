#!/usr/bin/env python3
"""
Comprehensive test suite for MCP server search functionality.
Consolidates all test cases from multiple test files.

Enhanced with rigorous quality metrics:
- Precision@K (how many of top K results are relevant)
- Mean Reciprocal Rank (position of first relevant result)
- False positive detection
- Relevance score analysis
- Failure categorization
"""

import asyncio
import sys
import json
import aiohttp
import statistics
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent))

from server.mcp_server import search_apple_docs, list_frameworks
from server.rag import SimpleRAG


@dataclass
class SearchMetrics:
    """Detailed metrics for search quality"""
    query: str
    platform: str
    first_relevant_position: Optional[int] = None  # 1-indexed
    precision_at_1: float = 0.0
    precision_at_3: float = 0.0
    precision_at_5: float = 0.0
    avg_relevance_score: float = 0.0
    top_result_score: float = 0.0
    num_results: int = 0
    has_false_positives: bool = False
    failure_category: Optional[str] = None
    search_time_ms: float = 0.0
    
    @property
    def reciprocal_rank(self) -> float:
        """Calculate reciprocal rank (1/position of first relevant result)"""
        if self.first_relevant_position:
            return 1.0 / self.first_relevant_position
        return 0.0
    
    @property
    def is_perfect(self) -> bool:
        """Check if this was a perfect search (relevant result at position 1)"""
        return self.first_relevant_position == 1
    
    @property
    def is_good(self) -> bool:
        """Check if this was a good search (relevant result in top 3)"""
        return self.first_relevant_position is not None and self.first_relevant_position <= 3
    
    @property 
    def is_acceptable(self) -> bool:
        """Check if this was acceptable (relevant result in top 5)"""
        return self.first_relevant_position is not None and self.first_relevant_position <= 5


async def test_rag_improvements():
    """Test RAG search improvements directly"""
    print("\n" + "=" * 80)
    print("üî¨ Testing RAG Search Improvements")
    print("=" * 80)
    
    rag = SimpleRAG()
    
    # Test cases covering all improvements
    test_cases = [
        # MCP patterns
        ("asyncimagephase failure in swiftui", "AsyncImagePhase.Failure", lambda r: "asyncimagephase/failure" in r['metadata'].get('file_path', '').lower()),
        ("view frame in swiftui", "frame() modifier", lambda r: "view/frame" in r['metadata'].get('file_path', '').lower()),
        ("alignment center in swiftui", "Alignment.center", lambda r: "alignment" in r['metadata'].get('file_path', '').lower() and "center" in r['metadata'].get('api_name', '').lower()),
        
        # Generic terms with context
        ("frame", "frame-related content", lambda r: "frame" in r['metadata'].get('api_name', '').lower() or "frame" in r['metadata'].get('title', '').lower()),
        ("init", "initializer documentation", lambda r: "init" in r['metadata'].get('api_name', '').lower()),
        ("TextField init SwiftUI", "TextField initializers", lambda r: "textfield" in r['metadata'].get('api_name', '').lower()),
        
        # Specific API searches
        ("Button SwiftUI", "Button struct", lambda r: "button" in r['metadata'].get('api_name', '').lower() and r['metadata'].get('framework') == 'SwiftUI'),
        ("URLSession async await", "URLSession with async/await", lambda r: "urlsession" in r['metadata'].get('api_name', '').lower() or "async" in r['metadata'].get('title', '').lower()),
    ]
    
    passed = 0
    for query, expected, check_fn in test_cases:
        print(f"\nüß™ Query: '{query}'")
        print(f"   Expected: {expected}")
        
        try:
            results = await rag.search(query, limit=5)
            
            # Check if expected result is in top 3
            found = any(check_fn(r) for r in results[:3])
            
            if found:
                print("   ‚úÖ PASSED - Found expected result in top 3")
                passed += 1
            else:
                print("   ‚ùå FAILED - Expected result not in top 3")
            
            # Show top result
            if results:
                r = results[0]
                print(f"   Top result: {r['metadata'].get('api_name', 'N/A')} ({r['metadata'].get('framework', 'N/A')})")
                print(f"   Score: {r.get('relevance_score', 0):.3f}, Boost: {r.get('boost_applied', False)}")
        
        except Exception as e:
            print(f"   ‚ùå ERROR: {e}")
    
    print(f"\nüìä RAG Test Results: {passed}/{len(test_cases)} passed ({passed/len(test_cases)*100:.1f}%)")
    return passed == len(test_cases)


async def test_mcp_protocol():
    """Test MCP server using HTTP protocol"""
    print("\n" + "=" * 80)
    print("üåê Testing MCP Server Protocol")
    print("=" * 80)
    
    base_url = "http://localhost:8080/mcp/v1"
    
    async with aiohttp.ClientSession() as session:
        # Check if server is running
        try:
            async with session.get(f"{base_url}/health") as resp:
                if resp.status != 200:
                    print("‚ùå MCP server not running or unhealthy")
                    print("   Run: cd mcp-server && make server")
                    return False
                print("‚úÖ MCP server is running")
        except:
            print("‚ùå Cannot connect to MCP server at localhost:8080")
            print("   Run: cd mcp-server && make server")
            return False
        
        # Test queries
        test_queries = [
            ("asyncimagephase failure in swiftui", "ios", "AsyncImagePhase.Failure"),
            ("frame", "ios", "frame-related content"),
            ("NSViewController", "macos", "NSViewController"),
        ]
        
        passed = 0
        for query, platform, expected in test_queries:
            print(f"\nüîç Query: '{query}' (platform: {platform})")
            print(f"   Expected: {expected}")
            
            request_data = {
                "jsonrpc": "2.0",
                "method": "tools/call",
                "params": {
                    "name": "search_apple_docs",
                    "arguments": {
                        "query": query,
                        "platform": platform,
                        "limit": 3
                    }
                },
                "id": 1
            }
            
            try:
                async with session.post(
                    f"{base_url}/call",
                    json=request_data,
                    headers={"Content-Type": "application/json"}
                ) as resp:
                    
                    if resp.status == 200:
                        result = await resp.json()
                        content = result.get("result", {}).get("content", [])
                        if content and isinstance(content[0], dict):
                            text = content[0].get("text", "")
                            if expected.lower() in text.lower():
                                print("   ‚úÖ PASSED - Found expected content")
                                passed += 1
                            else:
                                print("   ‚ùå FAILED - Expected content not found")
                        else:
                            print("   ‚ùå FAILED - Invalid response format")
                    else:
                        print(f"   ‚ùå Error: HTTP {resp.status}")
                        
            except Exception as e:
                print(f"   ‚ùå Request failed: {e}")
        
        print(f"\nüìä Protocol Test Results: {passed}/{len(test_queries)} passed ({passed/len(test_queries)*100:.1f}%)")
        return passed == len(test_queries)


async def calculate_search_metrics(
    query: str,
    platform: str,
    expected_terms: List[str],
    false_positive_terms: List[str] = None,
    expected_api_names: List[str] = None
) -> SearchMetrics:
    """Calculate detailed metrics for a single search"""
    
    metrics = SearchMetrics(query=query, platform=platform)
    false_positive_terms = false_positive_terms or []
    expected_api_names = expected_api_names or []
    
    try:
        start_time = time.time()
        result = await search_apple_docs(
            query=query,
            platform=platform,
            limit=10,  # Get more results for better analysis
            include_full_content=False
        )
        metrics.search_time_ms = (time.time() - start_time) * 1000
        
        result_lower = result.lower()
        lines = result.split('\n')
        
        # Parse results
        results_found = []
        relevance_scores = []
        
        for i, line in enumerate(lines):
            # Look for numbered results
            if line.strip().startswith(f"{len(results_found)+1}."):
                # Extract API name and score if available
                result_info = {
                    "position": len(results_found) + 1,
                    "line": line,
                    "is_relevant": False,
                    "is_false_positive": False
                }
                
                # Check if this result is relevant
                line_lower = line.lower()
                if any(term.lower() in line_lower for term in expected_terms):
                    result_info["is_relevant"] = True
                    if metrics.first_relevant_position is None:
                        metrics.first_relevant_position = result_info["position"]
                
                # Check for exact API name matches (higher quality)
                if expected_api_names:
                    for api_name in expected_api_names:
                        if api_name.lower() in line_lower:
                            result_info["is_relevant"] = True
                            if metrics.first_relevant_position is None:
                                metrics.first_relevant_position = result_info["position"]
                            break
                
                # Check for false positives
                if any(fp.lower() in line_lower for fp in false_positive_terms):
                    result_info["is_false_positive"] = True
                    metrics.has_false_positives = True
                
                results_found.append(result_info)
                
                # Extract relevance score if present
                if "relevance:" in lines[i+1].lower() if i+1 < len(lines) else False:
                    try:
                        score_line = lines[i+1]
                        score_str = score_line.split(":")[1].strip().rstrip('%')
                        relevance_scores.append(float(score_str) / 100)
                    except:
                        pass
        
        metrics.num_results = len(results_found)
        
        # Calculate precision at different K values
        for k in [1, 3, 5]:
            relevant_in_top_k = sum(1 for r in results_found[:k] if r["is_relevant"])
            if k <= len(results_found):
                setattr(metrics, f"precision_at_{k}", relevant_in_top_k / k)
        
        # Calculate average relevance score
        if relevance_scores:
            metrics.avg_relevance_score = statistics.mean(relevance_scores)
            metrics.top_result_score = relevance_scores[0] if relevance_scores else 0
        
        # Categorize failures
        if "no results found" in result_lower:
            metrics.failure_category = "no_results"
        elif metrics.first_relevant_position is None:
            if metrics.num_results > 0:
                metrics.failure_category = "irrelevant_results"
            else:
                metrics.failure_category = "no_results"
        elif metrics.first_relevant_position > 5:
            metrics.failure_category = "low_ranking"
        elif metrics.has_false_positives and metrics.first_relevant_position > 1:
            metrics.failure_category = "false_positive_interference"
            
    except Exception as e:
        metrics.failure_category = f"error: {str(e)[:50]}"
    
    return metrics


async def test_comprehensive_with_metrics():
    """Enhanced comprehensive test with detailed quality metrics"""
    
    print("\n" + "=" * 80)
    print("üß™ Comprehensive MCP Search Tests with Quality Metrics")
    print("=" * 80)
    
    # Enhanced test cases with expected API names and false positive detection
    test_categories = [
        {
            "name": "Exact API Name Searches",
            "tests": [
                {
                    "query": "UITableView",
                    "platform": "ios",
                    "expected_terms": ["UITableView", "tableview"],
                    "expected_apis": ["UITableView"],
                    "false_positives": ["UITableViewController", "UITableViewCell"],
                    "expect_exact_first": True
                },
                {
                    "query": "NSMutableArray",
                    "platform": "macos",
                    "expected_terms": ["NSMutableArray", "mutablearray"],
                    "expected_apis": ["NSMutableArray"],
                    "false_positives": ["NSArray", "NSMutableDictionary"],
                    "expect_exact_first": True
                },
                {
                    "query": "NavigationView",
                    "platform": "ios",
                    "expected_terms": ["NavigationView"],
                    "expected_apis": ["NavigationView"],
                    "false_positives": ["NavigationLink", "NavigationBarTitle"],
                    "expect_exact_first": True
                },
            ]
        },
        {
            "name": "Ambiguous Queries",
            "tests": [
                {
                    "query": "View",
                    "platform": "ios",
                    "expected_terms": ["View"],
                    "expected_apis": ["View", "UIView"],
                    "false_positives": [],
                    "expect_exact_first": False,
                    "note": "Should handle SwiftUI.View vs UIKit.UIView ambiguity"
                },
                {
                    "query": "Button",
                    "platform": "ios", 
                    "expected_terms": ["Button"],
                    "expected_apis": ["Button", "UIButton"],
                    "false_positives": ["ButtonStyle", "RadioButton"],
                    "expect_exact_first": False
                },
            ]
        },
        {
            "name": "MCP Pattern - Quality Check",
            "tests": [
                {
                    "query": "navigationview in swiftui",
                    "platform": "ios",
                    "expected_terms": ["navigationview", "NavigationView"],
                    "expected_apis": ["NavigationView"],
                    "false_positives": ["UINavigationController"],
                    "expect_exact_first": True
                },
                {
                    "query": "uitableviewcell in uikit",
                    "platform": "ios",
                    "expected_terms": ["uitableviewcell", "UITableViewCell"],
                    "expected_apis": ["UITableViewCell"],
                    "false_positives": ["UITableView", "UICollectionViewCell"],
                    "expect_exact_first": True
                },
            ]
        },
        {
            "name": "Common Misspellings",
            "tests": [
                {
                    "query": "tabel view",  # Missing 'l'
                    "platform": "ios",
                    "expected_terms": ["table", "tableview"],
                    "expected_apis": ["UITableView", "Table"],
                    "false_positives": [],
                    "expect_exact_first": False
                },
                {
                    "query": "navigtion",  # Missing 'a'
                    "platform": "ios",
                    "expected_terms": ["navigation"],
                    "expected_apis": ["Navigation", "UINavigation"],
                    "false_positives": [],
                    "expect_exact_first": False
                },
            ]
        },
        {
            "name": "Queries That Should Fail",
            "tests": [
                {
                    "query": "AndroidFragment",  # Non-existent API
                    "platform": "ios",
                    "expected_terms": [],
                    "expected_apis": [],
                    "false_positives": ["Fragment"],
                    "expect_exact_first": False,
                    "should_return_no_results": True
                },
                {
                    "query": "ReactComponent",  # Non-Apple API
                    "platform": "ios",
                    "expected_terms": [],
                    "expected_apis": [],
                    "false_positives": ["Component"],
                    "expect_exact_first": False,
                    "should_return_no_results": True
                },
            ]
        },
        {
            "name": "CamelCase APIs",
            "tests": [
                {
                    "query": "UITableView", 
                    "platform": "ios",
                    "expected_terms": ["UITableView", "tableview"],
                    "expected_apis": ["UITableView"],
                    "false_positives": []
                },
                {
                    "query": "NSMutableArray",
                    "platform": "macos", 
                    "expected_terms": ["NSMutableArray", "mutablearray"],
                    "expected_apis": ["NSMutableArray"],
                    "false_positives": []
                },
                {
                    "query": "AVPlayerViewController",
                    "platform": "ios",
                    "expected_terms": ["AVPlayerViewController", "player"],
                    "expected_apis": ["AVPlayerViewController"],
                    "false_positives": []
                },
                {
                    "query": "SKPaymentQueue",
                    "platform": "ios",
                    "expected_terms": ["SKPaymentQueue", "payment"],
                    "expected_apis": ["SKPaymentQueue"],
                    "false_positives": []
                },
                {
                    "query": "CLLocationManager",
                    "platform": "ios",
                    "expected_terms": ["CLLocationManager", "location"],
                    "expected_apis": ["CLLocationManager"],
                    "false_positives": []
                },
            ]
        },
        {
            "name": "Common Methods",
            "tests": [
                {
                    "query": "viewDidLoad",
                    "platform": "ios",
                    "expected_terms": ["viewDidLoad", "view"],
                    "expected_apis": ["viewDidLoad"],
                    "false_positives": []
                },
                {
                    "query": "layoutSubviews",
                    "platform": "ios",
                    "expected_terms": ["layoutSubviews", "layout"],
                    "expected_apis": ["layoutSubviews"],
                    "false_positives": []
                },
                {
                    "query": "drawRect",
                    "platform": "macos",
                    "expected_terms": ["drawRect", "draw"],
                    "expected_apis": ["drawRect"],
                    "false_positives": []
                },
                {
                    "query": "cellForRowAtIndexPath",
                    "platform": "ios",
                    "expected_terms": ["cellForRow", "cell"],
                    "expected_apis": ["cellForRowAtIndexPath"],
                    "false_positives": []
                },
                {
                    "query": "numberOfSections",
                    "platform": "ios",
                    "expected_terms": ["numberOfSections", "sections"],
                    "expected_apis": ["numberOfSections"],
                    "false_positives": []
                },
            ]
        },
        {
            "name": "Generic Terms - Quality Check", 
            "tests": [
                {
                    "query": "frame",
                    "platform": "ios",
                    "expected_terms": ["frame"],
                    "expected_apis": [],
                    "false_positives": [],
                    "should_show_tips": True,
                    "note": "Generic term should show search tips"
                },
                {
                    "query": "init",
                    "platform": "ios",
                    "expected_terms": [],
                    "expected_apis": [],
                    "false_positives": [],
                    "should_show_tips": True,
                    "note": "Too generic - should guide user"
                },
                {
                    "query": "frame modifier",
                    "platform": "ios",
                    "expected_terms": ["frame"],
                    "expected_apis": ["frame"],
                    "false_positives": [],
                    "note": "Context added - should work well"
                },
            ]
        },
        {
    ]
    
    # Track overall metrics
    all_metrics: List[SearchMetrics] = []
    category_metrics: Dict[str, List[SearchMetrics]] = defaultdict(list)
    failure_categories: Dict[str, int] = defaultdict(int)
    
    # Run tests with detailed metrics
    for category in test_categories:
        print(f"\n{'=' * 60}")
        print(f"üìã {category['name']}")
        print(f"{'=' * 60}")
        
        for test in category['tests']:
            query = test['query']
            platform = test['platform']
            expected_terms = test.get('expected_terms', [])
            expected_apis = test.get('expected_apis', [])
            false_positives = test.get('false_positives', [])
            
            print(f"\nüîç Query: '{query}' (platform: {platform})")
            if 'note' in test:
                print(f"   Note: {test['note']}")
            
            # Calculate detailed metrics
            metrics = await calculate_search_metrics(
                query=query,
                platform=platform,
                expected_terms=expected_terms,
                false_positive_terms=false_positives,
                expected_api_names=expected_apis
            )
            
            all_metrics.append(metrics)
            category_metrics[category['name']].append(metrics)
            
            # Display results
            if test.get('should_return_no_results', False):
                if metrics.num_results == 0 or metrics.failure_category == "no_results":
                    print("   ‚úÖ PASS - Correctly returned no results")
                else:
                    print("   ‚ùå FAIL - Should have returned no results")
            elif test.get('should_show_tips', False):
                if metrics.failure_category == "no_results":
                    print("   ‚úÖ PASS - Shows search tips as expected")
                else:
                    print("   ‚ö†Ô∏è  WARN - Expected search tips but got results")
            else:
                # Regular evaluation
                if metrics.is_perfect:
                    print("   üåü PERFECT - Relevant result at position #1")
                elif metrics.is_good:
                    print(f"   ‚úÖ GOOD - Relevant result at position #{metrics.first_relevant_position}")
                elif metrics.is_acceptable:
                    print(f"   ‚ö†Ô∏è  ACCEPTABLE - Relevant result at position #{metrics.first_relevant_position}")
                else:
                    print("   ‚ùå FAIL - No relevant results in top 5")
                    if metrics.failure_category:
                        failure_categories[metrics.failure_category] += 1
            
            # Quality indicators
            if test.get('expect_exact_first', False) and not metrics.is_perfect:
                print("   ‚ö†Ô∏è  Expected exact match at #1 but wasn't")
            
            if metrics.has_false_positives:
                print("   ‚ö†Ô∏è  False positives detected in results")
            
            # Performance metrics
            print(f"   Metrics: P@1={metrics.precision_at_1:.0%}, P@3={metrics.precision_at_3:.0%}, MRR={metrics.reciprocal_rank:.3f}")
            print(f"   Relevance: Top={metrics.top_result_score:.2f}, Avg={metrics.avg_relevance_score:.2f}")
            print(f"   Performance: {metrics.search_time_ms:.0f}ms, {metrics.num_results} results")
    
    # Calculate aggregate metrics
    print(f"\n{'=' * 80}")
    print("üìä AGGREGATE QUALITY METRICS")
    print(f"{'=' * 80}")
    
    # Overall performance
    perfect_searches = sum(1 for m in all_metrics if m.is_perfect)
    good_searches = sum(1 for m in all_metrics if m.is_good)
    acceptable_searches = sum(1 for m in all_metrics if m.is_acceptable)
    failed_searches = sum(1 for m in all_metrics if not m.is_acceptable and m.failure_category != "no_results")
    
    print(f"\nüéØ Search Quality Distribution:")
    print(f"   Perfect (result #1): {perfect_searches}/{len(all_metrics)} ({perfect_searches/len(all_metrics)*100:.1f}%)")
    print(f"   Good (top 3): {good_searches}/{len(all_metrics)} ({good_searches/len(all_metrics)*100:.1f}%)")
    print(f"   Acceptable (top 5): {acceptable_searches}/{len(all_metrics)} ({acceptable_searches/len(all_metrics)*100:.1f}%)")
    print(f"   Failed: {failed_searches}/{len(all_metrics)} ({failed_searches/len(all_metrics)*100:.1f}%)")
    
    # Mean metrics
    valid_metrics = [m for m in all_metrics if m.num_results > 0]
    if valid_metrics:
        mean_mrr = statistics.mean(m.reciprocal_rank for m in valid_metrics if m.reciprocal_rank > 0)
        mean_p_at_1 = statistics.mean(m.precision_at_1 for m in valid_metrics)
        mean_p_at_3 = statistics.mean(m.precision_at_3 for m in valid_metrics)
        mean_relevance = statistics.mean(m.avg_relevance_score for m in valid_metrics if m.avg_relevance_score > 0)
        mean_time = statistics.mean(m.search_time_ms for m in all_metrics)
        
        print(f"\nüìà Average Metrics:")
        print(f"   Mean Reciprocal Rank (MRR): {mean_mrr:.3f}")
        print(f"   Mean Precision@1: {mean_p_at_1:.1%}")
        print(f"   Mean Precision@3: {mean_p_at_3:.1%}")
        print(f"   Mean Relevance Score: {mean_relevance:.2f}")
        print(f"   Mean Search Time: {mean_time:.0f}ms")
    
    # Category breakdown
    print(f"\nüìä Performance by Category:")
    for cat_name, cat_metrics in category_metrics.items():
        perfect = sum(1 for m in cat_metrics if m.is_perfect)
        good = sum(1 for m in cat_metrics if m.is_good)
        print(f"   {cat_name}: {good}/{len(cat_metrics)} good ({good/len(cat_metrics)*100:.0f}%), {perfect} perfect")
    
    # Failure analysis
    if failure_categories:
        print(f"\n‚ùå Failure Analysis:")
        for failure_type, count in sorted(failure_categories.items(), key=lambda x: x[1], reverse=True):
            print(f"   {failure_type}: {count} occurrences")
    
    # Quality assessment
    print(f"\n‚úÖ Overall Assessment:")
    if mean_mrr >= 0.8:
        print("   üåü EXCELLENT - Very high quality search results")
    elif mean_mrr >= 0.6:
        print("   ‚úÖ GOOD - Search quality meets expectations")  
    elif mean_mrr >= 0.4:
        print("   ‚ö†Ô∏è  ACCEPTABLE - Search works but has room for improvement")
    else:
        print("   ‚ùå POOR - Significant search quality issues")
    
    # Recommendations
    print(f"\nüí° Recommendations:")
    if perfect_searches / len(all_metrics) < 0.5:
        print("   - Less than 50% of searches have perfect results (exact match at #1)")
        print("   - Consider improving exact match boosting or filename matching")
    
    if mean_time > 1000:
        print("   - Average search time exceeds 1 second")
        print("   - Consider performance optimizations")
    
    false_positive_queries = [m.query for m in all_metrics if m.has_false_positives and not m.is_perfect]
    if false_positive_queries:
        print(f"   - False positives affecting: {', '.join(false_positive_queries[:3])}...")
        print("   - Consider improving relevance scoring to reduce false positives")
    
    return all_metrics


async def test_comprehensive():
    """Original comprehensive test for backward compatibility"""
    
    print("\n" + "=" * 80)
    print("üß™ Comprehensive MCP Search Tests")
    print("=" * 80)
    
    # Run the enhanced test instead
    await test_comprehensive_with_metrics()


async def compare_before_after():
    """Compare search results before and after improvements"""
    print("\n" + "=" * 80)
    print("üîÑ Before/After Comparison (if backup exists)")
    print("=" * 80)
    
    backup_path = Path("server/rag_backup.py")
    if not backup_path.exists():
        print("   No backup file found - skipping comparison")
        return
    
    # Would implement comparison logic here
    print("   Backup comparison not implemented in consolidated version")


async def test_regression_suite():
    """Regression tests to ensure fixes don't break other queries"""
    print("\n" + "=" * 80)
    print("üîí Regression Test Suite")
    print("=" * 80)
    
    # Critical queries that must always work
    regression_tests = [
        ("Button", "ios", "Button"),
        ("navigationview in swiftui", "ios", "NavigationView"),
        ("URLSession", "all", "URLSession"),
        ("Text SwiftUI", "ios", "Text"),
        ("frame modifier", "ios", "frame"),
    ]
    
    passed = 0
    for query, platform, expected in regression_tests:
        try:
            result = await search_apple_docs(query, platform, limit=3)
            if expected.lower() in result.lower():
                passed += 1
                status = "‚úÖ"
            else:
                status = "‚ùå"
                print(f"\n   REGRESSION: '{query}' no longer returns '{expected}'")
        except:
            status = "‚ùå"
        
        print(f"   {status} {query}")
    
    print(f"\n   Regression suite: {passed}/{len(regression_tests)} passed")
    if passed < len(regression_tests):
        print("   ‚ö†Ô∏è  WARNING: Some regression tests failed!")


test_categories = [
        {
            "name": "Multi-word Natural Queries",
            "tests": [
                {
                    "query": "table view delegate methods",
                    "platform": "ios",
                    "expected_terms": ["table", "delegate"],
                    "expected_apis": ["UITableViewDelegate"],
                    "false_positives": []
                },
                {
                    "query": "collection view flow layout",
                    "platform": "ios",
                    "expected_terms": ["collection", "flow"],
                    "expected_apis": ["UICollectionViewFlowLayout"],
                    "false_positives": []
                },
                {
                    "query": "core data fetch request",
                    "platform": "all",
                    "expected_terms": ["core", "data", "fetch"],
                    "expected_apis": ["NSFetchRequest"],
                    "false_positives": []
                },
                {
                    "query": "navigation bar appearance",
                    "platform": "ios",
                    "expected_terms": ["navigation", "bar"],
                    "expected_apis": ["UINavigationBarAppearance"],
                    "false_positives": []
                },
                {
                    "query": "status bar style",
                    "platform": "ios",
                    "expected_terms": ["status", "bar"],
                    "expected_apis": ["UIStatusBarStyle"],
                    "false_positives": []
                },
            ]
        },
        {
            "name": "SwiftUI Specific",
            "tests": [
                ("GeometryReader", "ios", ["GeometryReader", "geometry"]),
                ("LazyVGrid", "ios", ["LazyVGrid", "grid"]),
                ("EnvironmentObject", "ios", ["EnvironmentObject", "environment"]),
                ("PreferenceKey", "ios", ["PreferenceKey", "preference"]),
                ("ViewBuilder", "ios", ["ViewBuilder", "builder"]),
            ]
        },
        {
            "name": "Generic Terms with Context",
            "tests": [
                ("background color", "ios", ["background", "color"]),
                ("font size", "ios", ["font", "size"]),
                ("corner radius", "ios", ["corner", "radius"]),
                ("shadow opacity", "ios", ["shadow", "opacity"]),
                ("border width", "ios", ["border", "width"]),
            ]
        },
        {
            "name": "Protocol and Delegate Patterns",
            "tests": [
                ("UITableViewDataSource", "ios", ["UITableViewDataSource", "datasource"]),
                ("NSURLSessionDelegate", "all", ["NSURLSessionDelegate", "delegate"]),
                ("UIGestureRecognizerDelegate", "ios", ["gesture", "delegate"]),
                ("UITextFieldDelegate", "ios", ["textfield", "delegate"]),
                ("UIScrollViewDelegate", "ios", ["scroll", "delegate"]),
            ]
        },
        {
            "name": "Framework-specific Searches",
            "tests": [
                ("ARSession", "ios", ["ARSession", "session"]),
                ("MLModel", "all", ["MLModel", "model"]),
                ("HKHealthStore", "ios", ["HKHealthStore", "health"]),
                ("MKMapView", "ios", ["MKMapView", "map"]),
                ("WKWebView", "ios", ["WKWebView", "web"]),
            ]
        },
        {
            "name": "Edge Cases",
            "tests": [
                ("UI", "ios", ["UI"]),  # Very short query
                ("NS", "macos", ["NS"]),  # Very short query
                ("@objc dynamic var", "all", ["objc", "dynamic"]),  # Special characters
                ("Swift.Result", "all", ["Swift", "Result"]),  # Dot notation
                ("async/await", "all", ["async", "await"]),  # Slash in query
            ]
        }
    ]
    
    total_tests = 0
    passed_tests = 0
    failed_queries = []
    
    for category in test_categories:
        print(f"\n{'=' * 60}")
        print(f"üìã {category['name']}")
        print(f"{'=' * 60}")
        
        for query, platform, expected_terms in category['tests']:
            total_tests += 1
            print(f"\nüîç Query: '{query}' (platform: {platform})")
            
            try:
                result = await search_apple_docs(
                    query=query,
                    platform=platform,
                    limit=3,
                    include_full_content=False
                )
                
                result_lower = result.lower()
                
                # Check if ANY expected term is in results
                found_any = any(term.lower() in result_lower for term in expected_terms)
                
                # For "No results", check if search tips are shown
                if "no results found" in result_lower and "search tips" in result_lower:
                    print("   ‚ÑπÔ∏è  No results (search tips shown)")
                    # This counts as a pass if it's a difficult query
                    if len(query) <= 2:  # Very short queries
                        passed_tests += 1
                        print("   ‚úÖ PASS - Appropriate for short query")
                    else:
                        print("   ‚ùå FAIL - Should have found results")
                        failed_queries.append((query, platform, "No results"))
                elif found_any:
                    print("   ‚úÖ PASS - Found relevant results")
                    passed_tests += 1
                    
                    # Show first result
                    lines = result.split('\n')
                    for line in lines:
                        if line.strip().startswith('1.'):
                            print(f"   First: {line.strip()[:80]}...")
                            break
                else:
                    print("   ‚ùå FAIL - No relevant results found")
                    failed_queries.append((query, platform, "Not relevant"))
                    
                    # Show what we got
                    lines = result.split('\n')
                    for line in lines:
                        if line.strip().startswith('1.'):
                            print(f"   Got: {line.strip()[:80]}...")
                            break
                
            except Exception as e:
                print(f"   ‚ùå ERROR: {str(e)[:100]}")
                failed_queries.append((query, platform, f"Error: {str(e)[:50]}"))
    
    # Summary
    print(f"\n{'=' * 80}")
    print(f"\nüìä COMPREHENSIVE TEST RESULTS")
    print(f"{'=' * 80}")
    print(f"\n   Total Tests: {total_tests}")
    print(f"   Passed: {passed_tests}")
    print(f"   Failed: {total_tests - passed_tests}")
    print(f"   Success Rate: {(passed_tests/total_tests)*100:.1f}%")
    
    if failed_queries:
        print(f"\n‚ùå Failed Queries ({len(failed_queries)}):")
        for query, platform, reason in failed_queries[:10]:  # Show first 10
            print(f"   - '{query}' on {platform}: {reason}")
        if len(failed_queries) > 10:
            print(f"   ... and {len(failed_queries) - 10} more")
    
    # Performance categories
    categories_performance = {}
    for category in test_categories:
        cat_total = len(category['tests'])
        cat_passed = sum(1 for q, p, e in category['tests'] 
                        if not any(q == fq and p == fp for fq, fp, _ in failed_queries))
        categories_performance[category['name']] = (cat_passed, cat_total)
    
    print("\nüìà Performance by Category:")
    for cat_name, (passed, total) in categories_performance.items():
        pct = (passed/total)*100 if total > 0 else 0
        print(f"   {cat_name}: {passed}/{total} ({pct:.0f}%)")
    
    print("\n‚úÖ Analysis:")
    if passed_tests/total_tests >= 0.8:
        print("   Excellent performance across diverse queries!")
    elif passed_tests/total_tests >= 0.6:
        print("   Good performance, but some query types need improvement")
    else:
        print("   Significant issues with search quality")


async def test_before_after_comparison():
    """Compare search results if backup exists"""
    print("\n" + "=" * 80)
    print("üî¨ Before/After Comparison Test")
    print("=" * 80)
    
    # Check if backup exists
    backup_path = Path(__file__).parent / "server" / "rag_backup.py"
    if not backup_path.exists():
        print("‚ö†Ô∏è  No backup file found for comparison")
        return True
    
    try:
        # Import both versions
        from server.rag import SimpleRAG as ImprovedRAG
        from server.rag_backup import SimpleRAG as OriginalRAG
        
        print("Comparing Original vs Improved RAG Search...")
        
        original = OriginalRAG()
        improved = ImprovedRAG()
        
        # Test queries
        test_queries = [
            ("asyncimagephase failure in swiftui", "asyncimagephase/failure"),
            ("init", "relevant init methods"),
            ("frame", "frame-related APIs"),
            ("button", "button"),
        ]
        
        improvements = 0
        for query, check_path in test_queries:
            print(f"\nüìä Query: '{query}'")
            
            original_results = await original.search(query, limit=5)
            improved_results = await improved.search(query, limit=5)
            
            # Score results
            def score_results(results, check):
                score = 0
                for i, r in enumerate(results):
                    path = r['metadata'].get('file_path', '').lower()
                    if check in path:
                        score += (5 - i) * 2
                return score
            
            original_score = score_results(original_results, check_path)
            improved_score = score_results(improved_results, check_path)
            
            print(f"   Original score: {original_score}")
            print(f"   Improved score: {improved_score}")
            
            if improved_score > original_score:
                print("   ‚úÖ IMPROVED!")
                improvements += 1
            elif improved_score < original_score:
                print("   ‚ùå WORSE!")
            else:
                print("   ‚ûñ SAME")
        
        print(f"\nüìä Comparison Results: {improvements}/{len(test_queries)} improved")
        return improvements >= len(test_queries) // 2
        
    except ImportError:
        print("‚ö†Ô∏è  Could not import backup for comparison")
        return True


async def test_search_tips():
    """Test that search tips are shown for poor queries"""
    print("\n" + "=" * 80)
    print("üí° Testing Search Tips")
    print("=" * 80)
    
    # Test queries that should show tips
    tip_queries = [
        ("zzznonexistentapi", "No results found"),
        ("frame", "frame"),  # Might have few results
    ]
    
    passed = 0
    for query, desc in tip_queries:
        print(f"\nüîç Query: '{query}'")
        try:
            result = await search_apple_docs(
                query=query,
                platform="ios",
                limit=5,
                include_full_content=False
            )
            
            if "Search Tips" in result or "Try using" in result:
                print("   ‚úÖ Search tips shown")
                passed += 1
            elif "No results found" in result:
                print("   ‚úÖ No results message shown")
                passed += 1
            else:
                print("   ‚ùå No search tips shown")
                
        except Exception as e:
            print(f"   ‚ùå ERROR: {e}")
    
    print(f"\nüìä Search Tips Test Results: {passed}/{len(tip_queries)} passed")
    return passed == len(tip_queries)


async def run_all_tests():
    """Run all test suites"""
    print("\n" + "=" * 80)
    print("üöÄ COMPREHENSIVE MCP SERVER TEST SUITE")
    print("=" * 80)
    print("\nThis combines all tests from:")
    print("- test_before_after.py")
    print("- test_final_improvements.py")
    print("- test_final_mcp.py")
    print("- test_mcp_directly.py")
    print("- test_mcp_tools.py")
    print("- test_search_improvements.py")
    print("- test_search_now.py")
    print("- test_search_quick.py")
    print("- test_comprehensive_search.py (original)")
    
    # Check if MCP server is needed
    print("\n‚ö†Ô∏è  Some tests require the MCP server to be running.")
    print("   If not running: cd mcp-server && make server")
    
    test_results = {}
    
    # Run all test suites
    print("\n" + "=" * 80)
    print("Running all tests...")
    print("=" * 80)
    
    # 1. RAG improvements test
    test_results['RAG Improvements'] = await test_rag_improvements()
    
    # 2. MCP Protocol test (requires server)
    test_results['MCP Protocol'] = await test_mcp_protocol()
    
    # 3. Comprehensive search patterns
    test_results['Comprehensive Search'] = await test_comprehensive()
    
    # 4. Before/after comparison
    test_results['Before/After Comparison'] = await test_before_after_comparison()
    
    # 5. Search tips test
    test_results['Search Tips'] = await test_search_tips()
    
    # Final summary
    print("\n" + "=" * 80)
    print("üìä FINAL TEST SUMMARY")
    print("=" * 80)
    
    total_passed = sum(1 for v in test_results.values() if v)
    total_tests = len(test_results)
    
    for test_name, passed in test_results.items():
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"\n{test_name}: {status}")
    
    print(f"\n\nOverall: {total_passed}/{total_tests} test suites passed ({total_passed/total_tests*100:.1f}%)")
    
    if total_passed == total_tests:
        print("\nüéâ All tests passed! MCP server is ready for deployment.")
    else:
        print(f"\n‚ö†Ô∏è  {total_tests - total_passed} test suites failed. Review before deployment.")
    
    print("\n‚úÖ Key Features Verified:")
    print("   - MCP pattern searches working")
    print("   - Platform filtering functional")
    print("   - Search tips shown for poor queries")
    print("   - Relevance scoring improved")
    print("   - Natural language queries supported")


if __name__ == "__main__":
    asyncio.run(run_all_tests())